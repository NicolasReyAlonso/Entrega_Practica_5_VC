{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26328653",
   "metadata": {},
   "source": [
    "# Tareas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039cbe8",
   "metadata": {},
   "source": [
    "## Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ac480",
   "metadata": {},
   "source": [
    "Detecci贸n de colores de la ropa y de si es hombre o mujer con un modelo (enrealidad 2) entrenado por nosotros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e347d",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911a47fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tf_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-15 10:29:24.337514: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPUs detectadas: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import io, color\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import kagglehub\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs detectadas:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47bd031",
   "metadata": {},
   "source": [
    "### Definici贸n de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ae2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_Gender = \"DatabaseGender\"\n",
    "dataset_emotions = kagglehub.dataset_download(\"jonathanoheix/face-expression-recognition-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a6468",
   "metadata": {},
   "source": [
    "### Cargar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9dadf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, image_size=(48, 48)):\n",
    "    X, y = [], []\n",
    "    labels = os.listdir(path)\n",
    "    for label in labels:\n",
    "        folder = os.path.join(path, label)\n",
    "        for file in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, image_size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_gender, y_gender = load_dataset(dataset_Gender, (64,64))\n",
    "X_emotion, y_emotion = load_dataset(\n",
    "    os.path.join(dataset_emotions, \"images/train\"), (48,48)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286d17a",
   "metadata": {},
   "source": [
    "### Normalizar y Etiquetar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1d07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_gender = X_gender / 255.0\n",
    "X_emotion = X_emotion / 255.0\n",
    "\n",
    "le_gender = LabelEncoder()\n",
    "y_gender = le_gender.fit_transform(y_gender)\n",
    "\n",
    "le_emotion = LabelEncoder()\n",
    "y_emotion = le_emotion.fit_transform(y_emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c372e14",
   "metadata": {},
   "source": [
    "### Crear modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca38d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30363c",
   "metadata": {},
   "source": [
    "### Aplicar kfold y Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3df421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tf_gpu/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763202576.587973   10401 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2140 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-11-15 10:29:38.353189: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 637452288 exceeds 10% of free system memory.\n",
      "2025-11-15 10:29:38.899621: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 637452288 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:29:40.224242: I external/local_xla/xla/service/service.cc:163] XLA service 0x7959c000b580 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-15 10:29:40.224262: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-11-15 10:29:40.256007: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-15 10:29:40.478988: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91501\n",
      "2025-11-15 10:29:40.580940: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-15 10:29:41.764982: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_856', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 39/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2067 - loss: 1.9116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763202584.440132   11019 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2664 - loss: 1.7926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:29:50.054884: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 159390720 exceeds 10% of free system memory.\n",
      "2025-11-15 10:29:50.179171: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 159390720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.3097 - loss: 1.7137 - val_accuracy: 0.3846 - val_loss: 1.5511\n",
      "Epoch 2/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3963 - loss: 1.5416 - val_accuracy: 0.4330 - val_loss: 1.4853\n",
      "Epoch 3/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4360 - loss: 1.4536 - val_accuracy: 0.4777 - val_loss: 1.3705\n",
      "Epoch 4/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4573 - loss: 1.3973 - val_accuracy: 0.4857 - val_loss: 1.3374\n",
      "Epoch 5/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4823 - loss: 1.3383 - val_accuracy: 0.4918 - val_loss: 1.3201\n",
      "Epoch 6/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4982 - loss: 1.2906 - val_accuracy: 0.5124 - val_loss: 1.2801\n",
      "Epoch 7/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5133 - loss: 1.2426 - val_accuracy: 0.5110 - val_loss: 1.2999\n",
      "Epoch 8/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5317 - loss: 1.2003 - val_accuracy: 0.5107 - val_loss: 1.2850\n",
      "Epoch 9/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5535 - loss: 1.1470 - val_accuracy: 0.5237 - val_loss: 1.2705\n",
      "Epoch 10/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5685 - loss: 1.1077 - val_accuracy: 0.5134 - val_loss: 1.2777\n",
      "\n",
      " Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:30:24.866154: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 637479936 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.3229 - loss: 1.6965 - val_accuracy: 0.4171 - val_loss: 1.5155\n",
      "Epoch 2/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4139 - loss: 1.5068 - val_accuracy: 0.4584 - val_loss: 1.4134\n",
      "Epoch 3/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4506 - loss: 1.4271 - val_accuracy: 0.4738 - val_loss: 1.3680\n",
      "Epoch 4/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4766 - loss: 1.3580 - val_accuracy: 0.4781 - val_loss: 1.3522\n",
      "Epoch 5/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4949 - loss: 1.3054 - val_accuracy: 0.4944 - val_loss: 1.3061\n",
      "Epoch 6/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5189 - loss: 1.2539 - val_accuracy: 0.4988 - val_loss: 1.3077\n",
      "Epoch 7/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5401 - loss: 1.1963 - val_accuracy: 0.5056 - val_loss: 1.2778\n",
      "Epoch 8/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5542 - loss: 1.1585 - val_accuracy: 0.5156 - val_loss: 1.2667\n",
      "Epoch 9/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5747 - loss: 1.1116 - val_accuracy: 0.5215 - val_loss: 1.2815\n",
      "Epoch 10/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5883 - loss: 1.0650 - val_accuracy: 0.5151 - val_loss: 1.2880\n",
      "\n",
      " Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.3200 - loss: 1.6959 - val_accuracy: 0.4002 - val_loss: 1.5265\n",
      "Epoch 2/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4172 - loss: 1.5032 - val_accuracy: 0.4511 - val_loss: 1.4113\n",
      "Epoch 3/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4540 - loss: 1.4155 - val_accuracy: 0.4715 - val_loss: 1.3598\n",
      "Epoch 4/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4838 - loss: 1.3503 - val_accuracy: 0.4825 - val_loss: 1.3537\n",
      "Epoch 5/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5065 - loss: 1.2903 - val_accuracy: 0.5005 - val_loss: 1.3073\n",
      "Epoch 6/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5252 - loss: 1.2498 - val_accuracy: 0.5102 - val_loss: 1.2782\n",
      "Epoch 7/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5418 - loss: 1.1906 - val_accuracy: 0.5144 - val_loss: 1.2727\n",
      "Epoch 8/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5664 - loss: 1.1333 - val_accuracy: 0.5002 - val_loss: 1.3101\n",
      "Epoch 9/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5802 - loss: 1.0967 - val_accuracy: 0.5099 - val_loss: 1.2991\n",
      "Epoch 10/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5944 - loss: 1.0601 - val_accuracy: 0.5170 - val_loss: 1.2913\n",
      "\n",
      " Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.3177 - loss: 1.7137 - val_accuracy: 0.4211 - val_loss: 1.5094\n",
      "Epoch 2/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4068 - loss: 1.5292 - val_accuracy: 0.4556 - val_loss: 1.4406\n",
      "Epoch 3/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4503 - loss: 1.4273 - val_accuracy: 0.4910 - val_loss: 1.3362\n",
      "Epoch 4/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4788 - loss: 1.3593 - val_accuracy: 0.4917 - val_loss: 1.3185\n",
      "Epoch 5/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5022 - loss: 1.2914 - val_accuracy: 0.5095 - val_loss: 1.2853\n",
      "Epoch 6/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5216 - loss: 1.2411 - val_accuracy: 0.5111 - val_loss: 1.2905\n",
      "Epoch 7/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5422 - loss: 1.1876 - val_accuracy: 0.5194 - val_loss: 1.2736\n",
      "Epoch 8/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5617 - loss: 1.1396 - val_accuracy: 0.5146 - val_loss: 1.2699\n",
      "Epoch 9/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5856 - loss: 1.0823 - val_accuracy: 0.5245 - val_loss: 1.2725\n",
      "Epoch 10/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6003 - loss: 1.0364 - val_accuracy: 0.5279 - val_loss: 1.2526\n",
      "\n",
      " Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.3202 - loss: 1.7009 - val_accuracy: 0.4159 - val_loss: 1.5199\n",
      "Epoch 2/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4172 - loss: 1.4971 - val_accuracy: 0.4755 - val_loss: 1.3838\n",
      "Epoch 3/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4623 - loss: 1.4011 - val_accuracy: 0.4839 - val_loss: 1.3347\n",
      "Epoch 4/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4889 - loss: 1.3348 - val_accuracy: 0.5003 - val_loss: 1.3013\n",
      "Epoch 5/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5106 - loss: 1.2795 - val_accuracy: 0.5137 - val_loss: 1.2844\n",
      "Epoch 6/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5375 - loss: 1.2174 - val_accuracy: 0.5144 - val_loss: 1.2681\n",
      "Epoch 7/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5545 - loss: 1.1611 - val_accuracy: 0.5177 - val_loss: 1.2552\n",
      "Epoch 8/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5790 - loss: 1.1015 - val_accuracy: 0.5173 - val_loss: 1.2714\n",
      "Epoch 9/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6006 - loss: 1.0489 - val_accuracy: 0.5193 - val_loss: 1.2745\n",
      "Epoch 10/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6213 - loss: 0.9980 - val_accuracy: 0.5194 - val_loss: 1.2989\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = X_emotion  # o X_gender\n",
    "y = y_emotion  # o y_gender\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    print(f\"\\n Fold {fold}\")\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = build_cnn(X.shape[1:], len(np.unique(y)))\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
    "    \n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e24cde",
   "metadata": {},
   "source": [
    "### Guardar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b8a534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"cnn_emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14a0c2",
   "metadata": {},
   "source": [
    "### Carga de Modelo\n",
    "El paso de guardar el modelo anterior no es necesario, pero en mi caso si ya que en macOS la camara tiene la tendencia de crashear el kernel de python y es necesario recargar todo y si tuviese que reentrenar el modelo en cada ocasi贸n no podr铆a hacer una defensa a tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57cff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"cnn_emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fffaa0",
   "metadata": {},
   "source": [
    "### Carga de filtros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abba6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtro_happy(frame):\n",
    "    # Aumentar brillo\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    h,s,v = cv2.split(hsv)\n",
    "    v = cv2.add(v, 50)  # subir brillo\n",
    "    v = np.clip(v, 0, 255)\n",
    "    final_hsv = cv2.merge((h,s,v))\n",
    "    return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def filtro_sad(frame):\n",
    "    # Tonos azules (enfriar imagen)\n",
    "    frame = cv2.addWeighted(frame, 0.7, np.zeros_like(frame), 0.3, 0)\n",
    "    frame[:,:,0] = cv2.add(frame[:,:,0], 50)  # azul m谩s intenso\n",
    "    return frame\n",
    "\n",
    "def filtro_angry(frame):\n",
    "    # Tonos rojos\n",
    "    frame = cv2.addWeighted(frame, 0.7, np.zeros_like(frame), 0.3, 0)\n",
    "    frame[:,:,2] = cv2.add(frame[:,:,2], 50)  # rojo m谩s intenso\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0e793",
   "metadata": {},
   "source": [
    "### Resoluci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "labels = le_emotion.classes_  # ej. ['angry', 'happy', 'sad', ...]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Procesar cada cara detectada\n",
    "    for (x, y, w, h) in faces:\n",
    "        # ROI de la cara\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Preprocesar ROI para el modelo\n",
    "        face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        resized = cv2.resize(face_rgb, (48, 48)) / 255.0\n",
    "        resized = np.expand_dims(resized, axis=0)\n",
    "\n",
    "        # Predicci贸n\n",
    "        preds = model.predict(resized, verbose=0)\n",
    "        label = labels[np.argmax(preds)]\n",
    "        confidence = np.max(preds)\n",
    "\n",
    "        # --- Aplicar filtro SOLO A LA CARA ---\n",
    "        if label == \"happy\":\n",
    "            face_roi = filtro_happy(face_roi)\n",
    "        elif label == \"sad\":\n",
    "            face_roi = filtro_sad(face_roi)\n",
    "        elif label == \"angry\":\n",
    "            face_roi = filtro_angry(face_roi)\n",
    "        # m谩s emociones...\n",
    "\n",
    "        # Reemplazar la ROI filtrada en el frame original\n",
    "        frame[y:y+h, x:x+w] = face_roi\n",
    "\n",
    "        # Dibujar label arriba de la cara\n",
    "        cv2.putText(frame, f\"{label} ({confidence*100:.1f}%)\", (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "        # Dibujar el recuadro de detecci贸n\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Emotion Filters\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
